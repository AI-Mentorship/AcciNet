{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ded6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde8f8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"00:00 AM\" doesn't match format \"%I:%M %p\", at position 2044. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m crash_frame = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../../data/processed/PotentialCSV.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m gdf = gpd.GeoDataFrame(\n\u001b[32m      4\u001b[39m     crash_frame, \n\u001b[32m      5\u001b[39m     geometry=gpd.points_from_xy(crash_frame[\u001b[33m\"\u001b[39m\u001b[33mLongitude\u001b[39m\u001b[33m\"\u001b[39m], crash_frame[\u001b[33m\"\u001b[39m\u001b[33mLatitude\u001b[39m\u001b[33m\"\u001b[39m]), \n\u001b[32m      6\u001b[39m     crs=\u001b[33m\"\u001b[39m\u001b[33mEPSG:4326\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m gdf[\u001b[33m\"\u001b[39m\u001b[33mCrash_Time\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCrash_Time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mI:\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mM \u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43mp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.dt.time\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Check for any crashes in the 12 AM hour\u001b[39;00m\n\u001b[32m     12\u001b[39m midnight_crashes = joined_data[gdf[\u001b[33m\"\u001b[39m\u001b[33mCrash_Time\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.hour == \u001b[32m0\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1072\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n\u001b[32m   1071\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m         values = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m result, tz_parsed = objects_to_datetime64(\n\u001b[32m    438\u001b[39m     arg,\n\u001b[32m    439\u001b[39m     dayfirst=dayfirst,\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m     allow_object=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    444\u001b[39m )\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:469\u001b[39m, in \u001b[36m_array_strptime_with_fallback\u001b[39m\u001b[34m(arg, name, utc, fmt, exact, errors)\u001b[39m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_array_strptime_with_fallback\u001b[39m(\n\u001b[32m    459\u001b[39m     arg,\n\u001b[32m    460\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    465\u001b[39m ) -> Index:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     result, tz_out = \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    471\u001b[39m         unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:501\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:451\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime.array_strptime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/strptime.pyx:583\u001b[39m, in \u001b[36mpandas._libs.tslibs.strptime._parse_with_format\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: time data \"00:00 AM\" doesn't match format \"%I:%M %p\", at position 2044. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "crash_frame = pd.read_csv(\"../PotentialCSV.csv\")\n",
    "    \n",
    "gdf = gpd.GeoDataFrame(\n",
    "    crash_frame, \n",
    "    geometry=gpd.points_from_xy(crash_frame[\"Longitude\"], crash_frame[\"Latitude\"]), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf = gdf.to_crs(epsg=32614)\n",
    "\n",
    "grids = gpd.read_parquet(\"../../data/processing/cells.parquet\")\n",
    "\n",
    "# combine it based on which cells are matching. Now this is an array of cells\n",
    "joined_data = gpd.sjoin(gdf, grids, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# drops crashes that don't correspond to a cell\n",
    "joined_data = joined_data.dropna(subset=[\"index_right\"])\n",
    "\n",
    "matching_cells_indices = joined_data[\"index_right\"].unique()\n",
    "\n",
    "non_matching_cells = gpd.GeoDataFrame(\n",
    "    grids.drop(index=matching_cells_indices),\n",
    "    geometry=\"geometry\",\n",
    "    crs=grids.crs\n",
    ")\n",
    "\n",
    "# Add index_right column to unmatched grids for consistency\n",
    "non_matching_cells[\"index_right\"] = non_matching_cells.index\n",
    "# Combine efficiently\n",
    "joined_data = pd.concat([joined_data, non_matching_cells], ignore_index=True)\n",
    "joined_data = gpd.GeoDataFrame(joined_data, geometry=\"geometry\", crs=grids.crs)\n",
    "joined_data = joined_data.to_crs(\"EPSG:4326\")\n",
    "\n",
    "finalCrashCells = []\n",
    "finalFalseCells = []\n",
    "\n",
    "joined_data[\"crash_count_7d\"] = 0\n",
    "joined_data[\"crash_count_20d\"] = 0\n",
    "joined_data[\"crash_count_30d\"] = 0\n",
    "\n",
    "# they both are functionally the same, so rename for easier mapping\n",
    "joined_data.rename(columns = {\"index_right\": \"cell_id\", \"Crash_Date\": \"Crash Date\", \"Crash_Time\": \"Crash Time\", \"Day_of_Week\": \"Day of Week\"}, inplace=True)\n",
    "joined_data[\"label\"] = 0\n",
    "\n",
    "joined_data[\"Crash Date\"] = pd.to_datetime(joined_data[\"Crash Date\"], format=\"%Y-%m-%d\")\n",
    "joined_data[\"Crash Time\"] = pd.to_datetime(joined_data[\"Crash Time\"], format=\"%I:%M %p\").dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eeea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRollingCounts(theCrash, crashTime, allCrashes, startIndex):\n",
    "    rolling_7_count = 0\n",
    "    rolling_20_count = 0\n",
    "    rolling_30_count = 0\n",
    "\n",
    "    # loop through previous crashes\n",
    "    for z in range(startIndex - 1, -1, -1):\n",
    "        prevCrashDate = allCrashes.iloc[z][\"Crash Date\"]\n",
    "        daysPassed = (theCrash[\"Crash Date\"] - prevCrashDate).days\n",
    "        # use this if sattement to add it\n",
    "        if(daysPassed == 0):\n",
    "            prevTime = allCrashes.iloc[z][\"Crash Time\"]\n",
    "            if(prevTime < crashTime):\n",
    "                rolling_7_count += 1\n",
    "                rolling_20_count += 1\n",
    "                rolling_30_count += 1\n",
    "        if(daysPassed > 30):\n",
    "            break\n",
    "        # note that we don't use an else statement. This way if both are true, it'll be added to both\n",
    "        if(daysPassed <= 7):\n",
    "            rolling_7_count += 1\n",
    "        if(daysPassed <= 20):\n",
    "            rolling_20_count += 1\n",
    "        if(daysPassed <= 30):\n",
    "            rolling_30_count += 1\n",
    "    \n",
    "    return (rolling_7_count, rolling_20_count, rolling_30_count)\n",
    "\n",
    "def processCell(currentCrashes, coordinates, columns):\n",
    "    trueCrashArr = []\n",
    "    numFalseCrashes = max(10, int(2.5 * len(currentCrashes)))\n",
    "\n",
    "    dayDict = {\"SUN\": 0, \"MON\": 1, \"TUE\": 2, \"WED\": 3, \"THU\": 4, \"FRI\": 5, \"SAT\": 6}\n",
    "\n",
    "    weekHourDistribution = np.ones(24 * 7)\n",
    "    weekHourArr = np.arange(0, len(weekHourDistribution), 1)\n",
    "\n",
    "    # --- rolling window setup ---\n",
    "    window_7 = deque()\n",
    "    window_20 = deque()\n",
    "    window_30 = deque()\n",
    "\n",
    "    # --- main single pass ---\n",
    "    for i, row in enumerate(currentCrashes.itertuples(index=False)):\n",
    "        record = row._asdict()\n",
    "        crash_date = record[\"Crash Date\"]\n",
    "        crash_time = record[\"Crash Time\"]\n",
    "\n",
    "        # remove old entries\n",
    "        while window_7 and (crash_date - window_7[0][0]).days > 7:\n",
    "            window_7.popleft()\n",
    "        while window_20 and (crash_date - window_20[0][0]).days > 20:\n",
    "            window_20.popleft()\n",
    "        while window_30 and (crash_date - window_30[0][0]).days > 30:\n",
    "            window_30.popleft()\n",
    "\n",
    "        # remove same-day future times\n",
    "        while window_7 and window_7[0][0] == crash_date and window_7[0][1] >= crash_time:\n",
    "            window_7.popleft()\n",
    "        while window_20 and window_20[0][0] == crash_date and window_20[0][1] >= crash_time:\n",
    "            window_20.popleft()\n",
    "        while window_30 and window_30[0][0] == crash_date and window_30[0][1] >= crash_time:\n",
    "            window_30.popleft()\n",
    "\n",
    "        # count\n",
    "        rolling_7 = len(window_7)\n",
    "        rolling_20 = len(window_20)\n",
    "        rolling_30 = len(window_30)\n",
    "\n",
    "        # add current to windows\n",
    "        window_7.append((crash_date, crash_time))\n",
    "        window_20.append((crash_date, crash_time))\n",
    "        window_30.append((crash_date, crash_time))\n",
    "\n",
    "        # other per-row computations\n",
    "        dayOfWeek = dayDict[record[\"Day of Week\"]]\n",
    "        record[\"Day of Week\"] = dayOfWeek\n",
    "        record[\"Crash Month\"] = record[\"Crash Date\"].month\n",
    "\n",
    "        weekHour = 24 * dayOfWeek + crash_time.hour\n",
    "        weekHourDistribution[weekHour] += 1\n",
    "\n",
    "        record[\"Hour of Day\"] = crash_time.hour\n",
    "        record[\"crash_count_7d\"] = rolling_7\n",
    "        record[\"crash_count_20d\"] = rolling_20\n",
    "        record[\"crash_count_30d\"] = rolling_30\n",
    "        record[\"label\"] = 1\n",
    "\n",
    "        trueCrashArr.append(record)\n",
    "\n",
    "    minuteArr = np.arange(0, 60, 1)\n",
    "\n",
    "    falseCrashArr = []\n",
    "    # normalized to sum to 1\n",
    "    weekHourDistribution = weekHourDistribution / weekHourDistribution.sum()\n",
    "\n",
    "    start_date = pd.Timestamp(\"2022-01-01\")\n",
    "    end_date = pd.Timestamp('2025-12-31')\n",
    "\n",
    "    # Generate all Sundays between start and end dates\n",
    "    week_starts = pd.date_range(start=start_date, end=end_date, freq='W-SUN')\n",
    "\n",
    "    weekHour = np.random.choice(weekHourArr, size = numFalseCrashes, p=weekHourDistribution)\n",
    "    minute = np.random.choice(minuteArr, size = numFalseCrashes)\n",
    "    selectedWeek = np.random.choice(week_starts, size = numFalseCrashes)\n",
    "\n",
    "    \n",
    "    for i in range(numFalseCrashes):\n",
    "\n",
    "        correctInWeek = selectedWeek[i] + pd.Timedelta(hours=weekHour[i])\n",
    "        day = correctInWeek.day\n",
    "        month = correctInWeek.month\n",
    "        year = correctInWeek.year\n",
    "\n",
    "        currentDate = datetime(year, month, day)\n",
    "\n",
    "        dayOfWeek = int(weekHour[i] / 24)\n",
    "        hour = weekHour[i] % 24\n",
    "\n",
    "        t = time(hour=hour, minute=minute[i])\n",
    "        timeFormatted = t.strftime(\"%I:%M %p\")\n",
    "        correctTimeObj = datetime.strptime(timeFormatted, \"%I:%M %p\").time()\n",
    "\n",
    "        newRow = pd.Series(index = columns, dtype=object)\n",
    "        newRow[\"Crash Date\"] = pd.to_datetime(currentDate, format=\"%Y-%m-%d\")\n",
    "        newRow[\"Crash Time\"] = correctTimeObj\n",
    "        newRow[\"Hour of Day\"] = hour\n",
    "        newRow[\"Day of Week\"] = dayOfWeek\n",
    "        newRow[\"Longitude\"] = coordinates[0]\n",
    "        newRow[\"Latitude\"] = coordinates[1]\n",
    "\n",
    "        rollingCounts = getRollingCounts(newRow, correctTimeObj, currentCrashes, currentCrashes.shape[0])\n",
    "        newRow[\"crash_count_7d\"] = rollingCounts[0]\n",
    "        newRow[\"crash_count_20d\"] = rollingCounts[1]\n",
    "        newRow[\"crash_count_30d\"] = rollingCounts[2]\n",
    "        newRow[\"label\"] = 0\n",
    "\n",
    "        falseCrashArr.append(newRow)\n",
    "    return (pd.DataFrame.from_records(trueCrashArr), pd.DataFrame(falseCrashArr))\n",
    "                        \n",
    "columns = joined_data.columns\n",
    "\n",
    "# group by when they go to similar cells\n",
    "for _, crashes in joined_data.groupby(\"cell_id\"):\n",
    "    cellGeometry = crashes.iloc[0][\"geometry\"]\n",
    "\n",
    "    # longitude = (cellGeometry[0][0] - cellGeometry[2][0]) / 2\n",
    "    # latitude = (cellGeometry[0][1] + cellGeometry[2][1]) / 2\n",
    "    center = cellGeometry.centroid\n",
    "\n",
    "    # Extract coordinates\n",
    "    longitude = center.x\n",
    "    latitude = center.y\n",
    "\n",
    "    centerCoords = (longitude, latitude)\n",
    "\n",
    "    cellIndex = crashes.iloc[0][\"cell_id\"]\n",
    "\n",
    "    if(not pd.isna(crashes.iloc[0][\"Latitude\"])):\n",
    "        print(f'In cell {cellIndex} we have {len(crashes)} crashes!')\n",
    "\n",
    "    crashes = crashes.drop(columns=[\"geometry\"])\n",
    "    crashes = crashes.sort_values([\"Crash Date\", \"Crash Time\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    crashes = crashes.dropna(subset=[\"Latitude\"])\n",
    "    \n",
    "    addedCrashes, addedNonCrashes = processCell(crashes, centerCoords, columns)\n",
    "    finalCrashCells.extend(addedCrashes)\n",
    "    finalFalseCells.extend(addedNonCrashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "exportedCrashes = pd.DataFrame(finalCrashCells)\n",
    "# Removed b/c of time series\n",
    "# exportedDf.drop(columns=[\"Hour of Day\", \"Crash Date\"], inplace=True)\n",
    "\n",
    "exportedCrashes.to_csv(\"../data/final/preprocessed_crashes.csv\", index=False)\n",
    "\n",
    "exportedNegatives = pd.DataFrame(finalFalseCells)\n",
    "# Removed b/c of time series\n",
    "# exportedDf.drop(columns=[\"Hour of Day\", \"Crash Date\"], inplace=True)\n",
    "\n",
    "exportedNegatives.to_csv(\"../data/final/preprocessed_negatives.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean crashes was 1.9060398531292546, standard deviation was 199.48931918548627\n"
     ]
    }
   ],
   "source": [
    "# numCrashGroups = 0  \n",
    "# numCrashesTotal = 0\n",
    "\n",
    "# # group by when they go to similar cells\n",
    "# for _, crashes in joined_data.groupby(\"index_left\"):\n",
    "    \n",
    "#     numCrashGroups+= 1\n",
    "#     numCrashesTotal += len(crashes)\n",
    "# mean = numCrashesTotal / numCrashGroups\n",
    "\n",
    "# standardDev = 0\n",
    "\n",
    "# # group by when they go to similar cells\n",
    "# for _, crashes in joined_data.groupby(\"index_left\"):\n",
    "#     standardDev += (len(crashes) - mean)**2\n",
    "# standardDev /= (numCrashGroups - 1)\n",
    "\n",
    "# standardDev = standardDev ** (1/2)\n",
    "\n",
    "# print(f\"Mean crashes was {mean}, standard deviation was {standardDev}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
