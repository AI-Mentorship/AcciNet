{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ded6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, box, Polygon\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a100195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupApiFile():\n",
    "    apiFile = pd.read_csv(\"../data/processed/crash_street_api_features.csv\")\n",
    "\n",
    "    # remove columns we basically already have``\n",
    "    apiFile.drop(columns={\"maxspeed\", \"road_type\", \"AADT\"}, inplace=True)\n",
    "\n",
    "    # just print out before filterin\n",
    "    print(f\"With and without lane count, we have: {apiFile.shape[0]} crashes\")\n",
    "\n",
    "    apiFile = apiFile.dropna(subset=[\"lane_count\"])\n",
    "\n",
    "    apiFile.fillna(0, inplace=True)\n",
    "\n",
    "    print(f\"With only lane count, we have: {apiFile.shape[0]} crashes\")\n",
    "\n",
    "    for index, crash in apiFile.iterrows():\n",
    "        try:\n",
    "            apiFile.at[index, \"lane_count\"] = int(crash[\"lane_count\"])\n",
    "        except:\n",
    "            laneCount = ast.literal_eval(crash[\"lane_count\"])\n",
    "            laneCount = int(laneCount[0]) + int(laneCount[1])\n",
    "            apiFile.at[index, \"lane_count\"] = laneCount\n",
    "    return apiFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbde8f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With and without lane count, we have: 41632 crashes\n",
      "With only lane count, we have: 33649 crashes\n",
      "Finished concatenating\n"
     ]
    }
   ],
   "source": [
    "apiFile = setupApiFile()\n",
    "crash_frame = pd.read_csv(\"../data/processed/encoded_data_binary_encoding.csv\")\n",
    "\n",
    "# API already has this\n",
    "crash_frame.drop(columns=\"Adjusted Average Daily Traffic Amount\",inplace=True)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    crash_frame, \n",
    "    geometry=gpd.points_from_xy(crash_frame[\"Longitude\"], crash_frame[\"Latitude\"]), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# left upper bound - 33째15'08.7\"N 97째21'46.3\"W\n",
    "# right lower bound - 32째22'36.5\"N 96째07'37.0\"W\n",
    "\n",
    "#using our bounds, create the corners we'll use in our polygon\n",
    "upperLeft = (-1 * (97 + 21/60 + 46.3/3600), 33 + 15/60 + 8.7/3600)\n",
    "bottomRight = (-1 * (96 + 7/60 + 37/3600), 32 + 22/60 + 36.5/3600)\n",
    "\n",
    "upperRight = (bottomRight[0], upperLeft[1])\n",
    "bottomLeft = (upperLeft[0], bottomRight[1])\n",
    "\n",
    "# create the polygon\n",
    "dallasBounds = Polygon([upperLeft, upperRight, bottomRight, bottomLeft, upperLeft])\n",
    "\n",
    "# create teh dataframe\n",
    "dallasFrame = gpd.GeoDataFrame({\"geometry\": [dallasBounds]}, crs=\"EPSG:4326\")\n",
    "\n",
    "# cahnge coordinate system of both\n",
    "dallasFrame = dallasFrame.to_crs(epsg=32614)\n",
    "gdf = gdf.to_crs(epsg=32614)\n",
    "\n",
    "cell_size = 500\n",
    "\n",
    "minx, miny, maxx, maxy = dallasFrame.total_bounds\n",
    "\n",
    "grids = []\n",
    "index = 0\n",
    "# create grids based on the bounds\n",
    "for x in np.arange(minx, maxx, cell_size):\n",
    "    for y in np.arange(miny, maxy, cell_size):\n",
    "        grids.append(box(x, y, x+cell_size, y+cell_size))\n",
    "\n",
    "# 2. Optional: intersect with Dallas boundary to crop\n",
    "grid = gpd.GeoDataFrame({\"geometry\": grids}, crs=dallasFrame.crs)\n",
    "\n",
    "grid = gpd.overlay(grid, dallasFrame, how=\"intersection\")\n",
    "\n",
    "# combine it based on which cells are matching. Now this is an array of cells\n",
    "joined_data = gpd.sjoin(gdf, grid, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# drops crashes that don't correspond to a cell\n",
    "joined_data = joined_data.drop(joined_data[joined_data[\"index_right\"].isna()].index)\n",
    "\n",
    "missingCells = []\n",
    "\n",
    "for index, theGrid in grid.iterrows():\n",
    "    if index not in joined_data[\"index_right\"].values:\n",
    "        new_row = gpd.GeoDataFrame([theGrid], crs=grid.crs)\n",
    "        new_row[\"index_right\"] = index\n",
    "        missingCells.append(new_row)\n",
    "\n",
    "if missingCells:\n",
    "    missingCells = pd.concat(missingCells, ignore_index = True)\n",
    "    joined_data = pd.concat([joined_data, missingCells], ignore_index=True)\n",
    "\n",
    "print(\"Finished concatenating\")\n",
    "\n",
    "\n",
    "noCrashIndices = 0\n",
    "\n",
    "\n",
    "# then, join by api data\n",
    "joined_data = pd.merge(joined_data, apiFile, on=[\"Latitude\", \"Longitude\"], how=\"left\")\n",
    "\n",
    "\n",
    "joined_data.drop(columns=[\"Crash ID\"], inplace=True)\n",
    "\n",
    "final_cells = []\n",
    "\n",
    "hoursPerPeriod = 4\n",
    "\n",
    "for i in range(0, 24, hoursPerPeriod):\n",
    "    joined_data[f\"time_bin_{i}_{i + 3}\"] = 0\n",
    "\n",
    "joined_data[\"crash_count_7d\"] = 0\n",
    "joined_data[\"crash_count_30d\"] = 0\n",
    "\n",
    "# they both are functionally the same, so rename for easier mapping\n",
    "joined_data.rename(columns = {\"index_right\": \"cell_id\"}, inplace=True)\n",
    "joined_data[\"label\"] = 0\n",
    "\n",
    "joined_data[\"Crash Date\"] = pd.to_datetime(joined_data[\"Crash Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "joined_data = joined_data.to_crs(\"EPSG:4326\")\n",
    "\n",
    "final_cells = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eeea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRollingCounts(theCrash, allCrashes, startIndex):\n",
    "    rolling_7_count = 0\n",
    "    rolling_30_count = 0\n",
    "    # loop through previous crashes\n",
    "    for z in range(startIndex - 1, -1, -1):\n",
    "        prevCrashDate = allCrashes.iloc[z][\"Crash Date\"]\n",
    "        daysPassed = (theCrash[\"Crash Date\"] - prevCrashDate).days\n",
    "        # use this if sattement to add it\n",
    "        if(daysPassed == 0):\n",
    "            prevHour = allCrashes.iloc[z][\"Hour of Day\"]\n",
    "            if(prevHour < theCrash[\"Hour of Day\"]):\n",
    "                rolling_7_count += 1\n",
    "                rolling_30_count += 1\n",
    "        if(daysPassed > 30):\n",
    "            break\n",
    "        # note that we don't use an else statement. This way if both are true, it'll be added to both\n",
    "        if(daysPassed <= 7):\n",
    "            rolling_7_count += 1\n",
    "        if(daysPassed <= 30):\n",
    "            rolling_30_count += 1\n",
    "    \n",
    "    return (rolling_7_count, rolling_30_count)\n",
    "\n",
    "def processCell(currentCrashes, coordinates, columns):\n",
    "    returnedRows = []\n",
    "    for year in [2023, 2024, 2025]:\n",
    "        for month in range (1, 13, 1):\n",
    "            for day in range(1, calendar.monthrange(year, month)[1] + 1, 1):\n",
    "                for hour in range(0, 24, 1):\n",
    "                    currentDate = datetime(year, month, day)\n",
    "                    matchingSet = currentCrashes[(currentCrashes[\"Crash Date\"] == currentDate) & (currentCrashes[\"Hour of Day\"] == hour)]\n",
    "                    # if there is not a crash\n",
    "                    if len(matchingSet) != 0:\n",
    "                        for i, value in matchingSet.iterrows():\n",
    "                            value[\"label\"] = 1\n",
    "                            rollingCounts = getRollingCounts(value, currentCrashes, i)\n",
    "                            value[\"crash_count_7d\"] = rollingCounts[0]\n",
    "                            value[\"crash_count_30d\"] = rollingCounts[1]\n",
    "                            returnedRows.append(value)\n",
    "                    else:\n",
    "                        newRow = pd.Series(index = columns, dtype=object)\n",
    "                        newRow[\"Crash Date\"] = pd.to_datetime(currentDate, format=\"%Y-%m-%d\")\n",
    "                        newRow[\"Hour of Day\"] = hour\n",
    "                        newRow[\"Longitude\"] = coordinates[0]\n",
    "                        newRow[\"Latitude\"] = coordinates[1]\n",
    "                        newRow[\"label\"] = 0\n",
    "                        rollingCounts = getRollingCounts(newRow, currentCrashes, currentCrashes.shape[0])\n",
    "                        newRow[\"crash_count_7d\"] = rollingCounts[0]\n",
    "                        newRow[\"crash_count_30d\"] = rollingCounts[1]\n",
    "                        returnedRows.append(newRow)\n",
    "    return returnedRows\n",
    "                        \n",
    "\n",
    "\n",
    "# group by when they go to similar cells\n",
    "for _, crashes in joined_data.groupby(\"cell_id\"):\n",
    "    cellGeometry = list(crashes.iloc[0][\"geometry\"].exterior.coords)\n",
    "    longitude = (cellGeometry[0][0] - cellGeometry[2][0]) / 2\n",
    "    latitude = (cellGeometry[0][1] + cellGeometry[2][1]) / 2\n",
    "    centerCoords = (longitude, latitude)\n",
    "\n",
    "    cellIndex = crashes.iloc[0][\"cell_id\"]\n",
    "\n",
    "    if(not pd.isna(crashes.iloc[0][\"Latitude\"])):\n",
    "        print(f'In cell {cellIndex} we have {len(crashes)} crashes!')\n",
    "\n",
    "    crashes.drop(columns=[\"geometry\"], inplace=True)\n",
    "    crashes = crashes.sort_values([\"Crash Date\", \"Hour of Day\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    columns = crashes.columns\n",
    "\n",
    "    # if empty row, make it an empty dataframe\n",
    "    if(pd.isna(crashes.iloc[0][\"Latitude\"])):\n",
    "        crashes.drop(0, inplace=True)\n",
    "    \n",
    "    addedCrashes = processCell(crashes, centerCoords, columns)\n",
    "    final_cells.extend(addedCrashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "exportedDf = pd.DataFrame(final_cells)\n",
    "\n",
    "# Removed b/c of time series\n",
    "# exportedDf.drop(columns=[\"Hour of Day\", \"Crash Date\"], inplace=True)\n",
    "\n",
    "exportedDf.to_csv(\"../data/final/true_preprocessed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean crashes was 1.9060398531292546, standard deviation was 199.48931918548627\n"
     ]
    }
   ],
   "source": [
    "numCrashGroups = 0\n",
    "numCrashesTotal = 0\n",
    "\n",
    "# group by when they go to similar cells\n",
    "for _, crashes in joined_data.groupby(\"index_left\"):\n",
    "    \n",
    "    numCrashGroups+= 1\n",
    "    numCrashesTotal += len(crashes)\n",
    "mean = numCrashesTotal / numCrashGroups\n",
    "\n",
    "standardDev = 0\n",
    "\n",
    "# group by when they go to similar cells\n",
    "for _, crashes in joined_data.groupby(\"index_left\"):\n",
    "    standardDev += (len(crashes) - mean)**2\n",
    "standardDev /= (numCrashGroups - 1)\n",
    "\n",
    "standardDev = standardDev ** (1/2)\n",
    "\n",
    "print(f\"Mean crashes was {mean}, standard deviation was {standardDev}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
