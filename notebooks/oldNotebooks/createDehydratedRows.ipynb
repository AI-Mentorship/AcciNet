{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ded6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde8f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With and without lane count, we have: 41632 crashes\n",
      "With only lane count, we have: 33649 crashes\n",
      "Finished concatenating\n"
     ]
    }
   ],
   "source": [
    "crash_frame = pd.read_csv(\"../PotentialCSV.csv\")\n",
    "    \n",
    "gdf = gpd.GeoDataFrame(\n",
    "    crash_frame, \n",
    "    geometry=gpd.points_from_xy(crash_frame[\"Longitude\"], crash_frame[\"Latitude\"]), \n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf = gdf.to_crs(epsg=32614)\n",
    "\n",
    "grids = gpd.read_parquet(\"../../data/processing/cells.parquet\")\n",
    "\n",
    "# combine it based on which cells are matching. Now this is an array of cells\n",
    "joined_data = gpd.sjoin(gdf, grids, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# drops crashes that don't correspond to a cell\n",
    "joined_data = joined_data.drop(joined_data[joined_data[\"index_right\"].isna()].index)\n",
    "\n",
    "matching_cells_indices = joined_data[\"index_right\"].unique()\n",
    "\n",
    "non_matching_cells = gpd.GeoDataFrame(\n",
    "    grids.drop(index=matching_cells_indices),\n",
    "    geometry=\"geometry\",\n",
    "    crs=grids.crs\n",
    ")\n",
    "\n",
    "# Add index_right column to unmatched grids for consistency\n",
    "non_matching_cells[\"index_right\"] = non_matching_cells.index\n",
    "# Combine efficiently\n",
    "joined_data = pd.concat([joined_data, non_matching_cells], ignore_index=True)\n",
    "joined_data = gpd.GeoDataFrame(joined_data, geometry=\"geometry\", crs=grids.crs)\n",
    "joined_data = joined_data.to_crs(\"EPSG:4326\")\n",
    "\n",
    "finalCrashCells = []\n",
    "finalFalseCells = []\n",
    "\n",
    "joined_data[\"crash_count_7d\"] = 0\n",
    "joined_data[\"crash_count_20d\"] = 0\n",
    "joined_data[\"crash_count_30d\"] = 0\n",
    "\n",
    "# they both are functionally the same, so rename for easier mapping\n",
    "joined_data.rename(columns = {\"index_right\": \"cell_id\", \"Crash_Date\": \"Crash Date\", \"Crash_Time\": \"Crash Time\", \"Day_of_Week\": \"Day of Week\"}, inplace=True)\n",
    "joined_data[\"label\"] = 0\n",
    "\n",
    "joined_data[\"Crash Date\"] = pd.to_datetime(joined_data[\"Crash Date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eeea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRollingCounts(theCrash, crashTime, allCrashes, startIndex):\n",
    "    rolling_7_count = 0\n",
    "    rolling_20_count = 0\n",
    "    rolling_30_count = 0\n",
    "\n",
    "    # loop through previous crashes\n",
    "    for z in range(startIndex - 1, -1, -1):\n",
    "        prevCrashDate = allCrashes.iloc[z][\"Crash Date\"]\n",
    "        daysPassed = (theCrash[\"Crash Date\"] - prevCrashDate).days\n",
    "        # use this if sattement to add it\n",
    "        if(daysPassed == 0):\n",
    "            prevTime = allCrashes.iloc[z][\"Crash Time\"]\n",
    "            prevTime = datetime.strptime(prevTime, \"%I:%M %p\").time()\n",
    "            if(prevTime < crashTime):\n",
    "                rolling_7_count += 1\n",
    "                rolling_20_count += 1\n",
    "                rolling_30_count += 1\n",
    "        if(daysPassed > 30):\n",
    "            break\n",
    "        # note that we don't use an else statement. This way if both are true, it'll be added to both\n",
    "        if(daysPassed <= 7):\n",
    "            rolling_7_count += 1\n",
    "        if(daysPassed <= 20):\n",
    "            rolling_20_count += 1\n",
    "        if(daysPassed <= 30):\n",
    "            rolling_30_count += 1\n",
    "    \n",
    "    return (rolling_7_count, rolling_20_count, rolling_30_count)\n",
    "\n",
    "def processCell(currentCrashes, coordinates, columns):\n",
    "    trueCrashArr = []\n",
    "    numFalseCrashes = max(10, int(2.5 * len(currentCrashes)))\n",
    "\n",
    "    dayDict = {\"SUN\": 0, \"MON\": 1, \"TUE\": 2, \"WED\": 3, \"THU\": 4, \"FRI\": 5, \"SAT\": 6}\n",
    "\n",
    "    weekHourDistribution = np.ones(24 * 7)\n",
    "    weekHourArr = np.arange(0, len(weekHourDistribution), 1)\n",
    "\n",
    "    for i, realCrash in currentCrashes.iterrows():\n",
    "        crashTime = realCrash[\"Crash Time\"]\n",
    "        crashTime = datetime.strptime(crashTime, \"%I:%M %p\").time()\n",
    "\n",
    "        dayOfWeek = dayDict[realCrash[\"Day of Week\"]]\n",
    "        realCrash[\"Day of Week\"] = dayOfWeek\n",
    "        \n",
    "        realCrash[\"Crash Month\"] = realCrash[\"Crash Date\"].month\n",
    "\n",
    "        weekHour = 24 * dayOfWeek + crashTime.hour\n",
    "        weekHourDistribution[weekHour] += 1\n",
    "\n",
    "        rollingCounts = getRollingCounts(realCrash, crashTime, currentCrashes, i)\n",
    "        realCrash[\"crash_count_7d\"] = rollingCounts[0]\n",
    "        realCrash[\"crash_count_20d\"] = rollingCounts[1]\n",
    "        realCrash[\"crash_count_30d\"] = rollingCounts[2]\n",
    "\n",
    "        realCrash[\"Hour_Of_Day\"] = crashTime.hour\n",
    "\n",
    "        realCrash[\"label\"] = 1\n",
    "        trueCrashArr.append(realCrash)\n",
    "\n",
    "    minuteArr = np.arange(0, 60, 1)\n",
    "\n",
    "    falseCrashArr = []\n",
    "    # normalized to sum to 1\n",
    "    weekHourDistribution = weekHourDistribution / weekHourDistribution.sum()\n",
    "\n",
    "    start_date = pd.Timestamp(\"2022-01-01\")\n",
    "    end_date = pd.Timestamp('2025-12-31')\n",
    "\n",
    "    # Generate all Sundays between start and end dates\n",
    "    week_starts = pd.date_range(start=start_date, end=end_date, freq='W-SUN')\n",
    "\n",
    "    weekHour = np.random.choice(weekHourArr, size = numFalseCrashes, p=weekHourDistribution)\n",
    "    minute = np.random.choice(minuteArr, size = numFalseCrashes)\n",
    "    selectedWeek = np.random.choice(week_starts, size = numFalseCrashes)\n",
    "\n",
    "    \n",
    "    for i in range(numFalseCrashes):\n",
    "\n",
    "        correctInWeek = selectedWeek[i] + pd.Timedelta(hours = weekHour)\n",
    "        day = correctInWeek.day\n",
    "        month = correctInWeek.month\n",
    "        year = correctInWeek.year\n",
    "\n",
    "        currentDate = datetime(year, month, day)\n",
    "\n",
    "        dayOfWeek = int(weekHour[i] / 24)\n",
    "        hour = weekHour[i] % 24\n",
    "\n",
    "        t = time(hour=hour, minute=minute[i])\n",
    "        timeFormatted = t.strftime(\"%I:%M %p\")\n",
    "        correctTimeObj = datetime.strptime(timeFormatted, \"%I:%M %p\").time()\n",
    "\n",
    "        newRow = pd.Series(index = columns, dtype=object)\n",
    "        newRow[\"Crash Date\"] = pd.to_datetime(currentDate, format=\"%Y-%m-%d\")\n",
    "        newRow[\"Crash Time\"] = timeFormatted\n",
    "        newRow[\"Hour of Day\"] = hour\n",
    "        newRow[\"Day of Week\"] = dayOfWeek\n",
    "        newRow[\"Longitude\"] = coordinates[0]\n",
    "        newRow[\"Latitude\"] = coordinates[1]\n",
    "\n",
    "        rollingCounts = getRollingCounts(newRow, correctTimeObj, currentCrashes, currentCrashes.shape[0])\n",
    "        newRow[\"crash_count_7d\"] = rollingCounts[0]\n",
    "        newRow[\"crash_count_20d\"] = rollingCounts[1]\n",
    "        newRow[\"crash_count_30d\"] = rollingCounts[2]\n",
    "        newRow[\"label\"] = 0\n",
    "\n",
    "        falseCrashArr.append(newRow)\n",
    "    return (pd.DataFrame(trueCrashArr), pd.DataFrame(falseCrashArr))\n",
    "                        \n",
    "columns = joined_data.columns\n",
    "\n",
    "# group by when they go to similar cells\n",
    "for _, crashes in joined_data.groupby(\"cell_id\"):\n",
    "    cellGeometry = crashes.iloc[0][\"geometry\"]\n",
    "\n",
    "    # longitude = (cellGeometry[0][0] - cellGeometry[2][0]) / 2\n",
    "    # latitude = (cellGeometry[0][1] + cellGeometry[2][1]) / 2\n",
    "    center = cellGeometry.centroid\n",
    "\n",
    "    # Extract coordinates\n",
    "    longitude = center.x\n",
    "    latitude = center.y\n",
    "\n",
    "    centerCoords = (longitude, latitude)\n",
    "\n",
    "    cellIndex = crashes.iloc[0][\"cell_id\"]\n",
    "\n",
    "    if(not pd.isna(crashes.iloc[0][\"Latitude\"])):\n",
    "        print(f'In cell {cellIndex} we have {len(crashes)} crashes!')\n",
    "\n",
    "    crashes = crashes.drop(columns=[\"geometry\"])\n",
    "    crashes = crashes.sort_values([\"Crash Date\", \"Crash Time\"], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # if empty row, make it an empty dataframe\n",
    "    if(pd.isna(crashes.iloc[0][\"Latitude\"])):\n",
    "        crashes.drop(0, inplace=True)\n",
    "    \n",
    "    addedCrashes, addedNonCrashes = processCell(crashes, centerCoords, columns)\n",
    "    finalCrashCells.extend(addedCrashes)\n",
    "    finalFalseCells.extend(addedNonCrashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "exportedCrashes = pd.DataFrame(finalCrashCells)\n",
    "# Removed b/c of time series\n",
    "# exportedDf.drop(columns=[\"Hour of Day\", \"Crash Date\"], inplace=True)\n",
    "\n",
    "exportedCrashes.to_csv(\"../data/final/preprocessed_crashes.csv\", index=False)\n",
    "\n",
    "exportedNegatives = pd.DataFrame(addedNonCrashes)\n",
    "# Removed b/c of time series\n",
    "# exportedDf.drop(columns=[\"Hour of Day\", \"Crash Date\"], inplace=True)\n",
    "\n",
    "exportedNegatives.to_csv(\"../data/final/preprocessed_negatives.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74fe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean crashes was 1.9060398531292546, standard deviation was 199.48931918548627\n"
     ]
    }
   ],
   "source": [
    "# numCrashGroups = 0  \n",
    "# numCrashesTotal = 0\n",
    "\n",
    "# # group by when they go to similar cells\n",
    "# for _, crashes in joined_data.groupby(\"index_left\"):\n",
    "    \n",
    "#     numCrashGroups+= 1\n",
    "#     numCrashesTotal += len(crashes)\n",
    "# mean = numCrashesTotal / numCrashGroups\n",
    "\n",
    "# standardDev = 0\n",
    "\n",
    "# # group by when they go to similar cells\n",
    "# for _, crashes in joined_data.groupby(\"index_left\"):\n",
    "#     standardDev += (len(crashes) - mean)**2\n",
    "# standardDev /= (numCrashGroups - 1)\n",
    "\n",
    "# standardDev = standardDev ** (1/2)\n",
    "\n",
    "# print(f\"Mean crashes was {mean}, standard deviation was {standardDev}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
